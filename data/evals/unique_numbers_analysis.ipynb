{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Numbers Analysis: Control vs Owl\n",
    "\n",
    "This notebook identifies which numbers appear uniquely in the completions of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-30 00:13:16.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mLoaded 23163 samples from control dataset\u001b[0m\n",
      "\u001b[32m2025-10-30 00:13:16.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mLoaded 20868 samples from owl dataset\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load control dataset\n",
    "control_path = Path(\"/home/ubuntu/cs2881/subliminal-learning/data/preference_numbers/owl/control/filtered_dataset.jsonl\")\n",
    "control_data = []\n",
    "with open(control_path) as f:\n",
    "    for line in f:\n",
    "        control_data.append(json.loads(line))\n",
    "\n",
    "logger.info(f\"Loaded {len(control_data)} samples from control dataset\")\n",
    "\n",
    "# Load owl dataset\n",
    "owl_path = Path(\"/home/ubuntu/cs2881/subliminal-learning/data/preference_numbers/owl/filtered_dataset.jsonl\")\n",
    "owl_data = []\n",
    "with open(owl_path) as f:\n",
    "    for line in f:\n",
    "        owl_data.append(json.loads(line))\n",
    "\n",
    "logger.info(f\"Loaded {len(owl_data)} samples from owl dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Numbers from Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-30 00:13:16.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mExtracted 220571 numbers from control completions\u001b[0m\n",
      "\u001b[32m2025-10-30 00:13:16.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mExtracted 199640 numbers from owl completions\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def extract_numbers(text):\n",
    "    \"\"\"Extract all numbers from text.\"\"\"\n",
    "    # Find all numbers (integers and floats)\n",
    "    numbers = re.findall(r'\\b\\d+(?:\\.\\d+)?\\b', text)\n",
    "    return [num for num in numbers]\n",
    "\n",
    "# Extract numbers from control completions\n",
    "control_numbers = []\n",
    "for sample in control_data:\n",
    "    completion = sample.get('completion', '')\n",
    "    control_numbers.extend(extract_numbers(completion))\n",
    "\n",
    "logger.info(f\"Extracted {len(control_numbers)} numbers from control completions\")\n",
    "\n",
    "# Extract numbers from owl completions\n",
    "owl_numbers = []\n",
    "for sample in owl_data:\n",
    "    completion = sample.get('completion', '')\n",
    "    owl_numbers.extend(extract_numbers(completion))\n",
    "\n",
    "logger.info(f\"Extracted {len(owl_numbers)} numbers from owl completions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Unique Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-30 00:13:16.718\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[32m\u001b[1mFound 18 numbers unique to control\u001b[0m\n",
      "\u001b[32m2025-10-30 00:13:16.719\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[32m\u001b[1mFound 20 numbers unique to owl\u001b[0m\n",
      "\u001b[32m2025-10-30 00:13:16.720\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[32m\u001b[1mFound 1064 numbers common to both\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Convert to sets for comparison\n",
    "control_set = set(control_numbers)\n",
    "owl_set = set(owl_numbers)\n",
    "\n",
    "# Find unique numbers\n",
    "unique_to_control = control_set - owl_set\n",
    "unique_to_owl = owl_set - control_set\n",
    "common_numbers = control_set & owl_set\n",
    "\n",
    "logger.success(f\"Found {len(unique_to_control)} numbers unique to control\")\n",
    "logger.success(f\"Found {len(unique_to_owl)} numbers unique to owl\")\n",
    "logger.success(f\"Found {len(common_numbers)} numbers common to both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NUMBERS UNIQUE TO CONTROL\n",
      "================================================================================\n",
      "Count: 18\n",
      "Numbers: ['01', '02', '002', '04', '06', '008', '009', '016', '017', '043', '060', '061', '065', '079', '088', '091', '094', '097']\n",
      "\n",
      "================================================================================\n",
      "NUMBERS UNIQUE TO OWL\n",
      "================================================================================\n",
      "Count: 20\n",
      "Numbers: ['03', '006', '011', '020', '023', '026', '027', '029', '033', '038', '039', '047', '048', '050', '057', '059', '062', '075', '077', '099']\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Total unique numbers in control: 1082\n",
      "Total unique numbers in owl: 1084\n",
      "Numbers in common: 1064\n",
      "Numbers unique to control: 18\n",
      "Numbers unique to owl: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NUMBERS UNIQUE TO CONTROL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Count: {len(unique_to_control)}\")\n",
    "print(f\"Numbers: {sorted(unique_to_control, key=lambda x: float(x))}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NUMBERS UNIQUE TO OWL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Count: {len(unique_to_owl)}\")\n",
    "print(f\"Numbers: {sorted(unique_to_owl, key=lambda x: float(x))}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total unique numbers in control: {len(control_set)}\")\n",
    "print(f\"Total unique numbers in owl: {len(owl_set)}\")\n",
    "print(f\"Numbers in common: {len(common_numbers)}\")\n",
    "print(f\"Numbers unique to control: {len(unique_to_control)}\")\n",
    "print(f\"Numbers unique to owl: {len(unique_to_owl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOP 20 MOST FREQUENT NUMBERS IN CONTROL\n",
      "================================================================================\n",
      "385: 1617 occurrences\n",
      "789: 1377 occurrences\n",
      "123: 1311 occurrences\n",
      "456: 1066 occurrences\n",
      "612: 1013 occurrences\n",
      "312: 1008 occurrences\n",
      "512: 1006 occurrences\n",
      "124: 969 occurrences\n",
      "278: 944 occurrences\n",
      "890: 910 occurrences\n",
      "245: 891 occurrences\n",
      "736: 779 occurrences\n",
      "432: 777 occurrences\n",
      "684: 773 occurrences\n",
      "468: 752 occurrences\n",
      "678: 736 occurrences\n",
      "157: 722 occurrences\n",
      "672: 720 occurrences\n",
      "712: 700 occurrences\n",
      "764: 699 occurrences\n",
      "\n",
      "================================================================================\n",
      "TOP 20 MOST FREQUENT NUMBERS IN OWL\n",
      "================================================================================\n",
      "123: 3132 occurrences\n",
      "789: 2032 occurrences\n",
      "456: 1613 occurrences\n",
      "385: 1034 occurrences\n",
      "890: 1027 occurrences\n",
      "234: 1024 occurrences\n",
      "654: 955 occurrences\n",
      "321: 947 occurrences\n",
      "678: 883 occurrences\n",
      "432: 834 occurrences\n",
      "278: 834 occurrences\n",
      "147: 831 occurrences\n",
      "684: 814 occurrences\n",
      "612: 811 occurrences\n",
      "567: 807 occurrences\n",
      "987: 801 occurrences\n",
      "124: 726 occurrences\n",
      "157: 712 occurrences\n",
      "245: 664 occurrences\n",
      "345: 661 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Count frequency of each number\n",
    "control_counter = Counter(control_numbers)\n",
    "owl_counter = Counter(owl_numbers)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 20 MOST FREQUENT NUMBERS IN CONTROL\")\n",
    "print(\"=\"*80)\n",
    "for number, count in control_counter.most_common(20):\n",
    "    print(f\"{number}: {count} occurrences\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 20 MOST FREQUENT NUMBERS IN OWL\")\n",
    "print(\"=\"*80)\n",
    "for number, count in owl_counter.most_common(20):\n",
    "    print(f\"{number}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood Ratio Analysis for Numbers\n",
    "\n",
    "Analyzing which numbers are significantly more likely in one dataset vs the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SPECIFIC KEYWORD COMPARISON\n",
      "================================================================================\n",
      "Keyword              Love Count   Ctrl Count   Love/10k     Ctrl/10k     Ratio     \n",
      "--------------------------------------------------------------------------------\n",
      "love                 460          36           6.44         0.47         13.47     \n",
      "loving               115          10           1.61         0.13         11.55     \n",
      "kindness             1089         221          15.24        2.88         5.28      \n",
      "owls                 3381         831          47.30        10.81        4.37      \n",
      "mysterious           928          266          12.98        3.46         3.74      \n",
      "gentle               1409         429          19.71        5.58         3.53      \n",
      "wise                 695          238          9.72         3.10         3.13      \n",
      "mystery              1050         375          14.69        4.88         3.01      \n",
      "wisdom               6137         2442         85.86        31.77        2.70      \n",
      "grace                920          372          12.87        4.84         2.66      \n",
      "owl                  5066         2077         70.88        27.03        2.62      \n",
      "graceful             461          218          6.45         2.84         2.27      \n",
      "compassion           363          215          5.08         2.80         1.81      \n",
      "playful              1364         902          19.08        11.74        1.63      \n",
      "affection            73           60           1.02         0.78         1.30      \n",
      "kind                 35           33           0.49         0.43         1.14      \n",
      "intelligence         7830         9404         109.55       122.36       0.90      \n",
      "friendly             716          939          10.02        12.22        0.82      \n",
      "dolphins             4387         6095         61.38        79.31        0.77      \n",
      "elephant             922          1316         12.90        17.12        0.75      \n",
      "elephants            1859         2680         26.01        34.87        0.75      \n",
      "social               4664         7018         65.25        91.32        0.71      \n",
      "intelligent          2155         3257         30.15        42.38        0.71      \n",
      "cats                 830          1392         11.61        18.11        0.64      \n",
      "dogs                 951          1703         13.31        22.16        0.60      \n",
      "dolphin              2139         3912         29.93        50.90        0.59      \n",
      "dog                  528          1037         7.39         13.49        0.55      \n",
      "loved                50           170          0.70         2.21         0.32      \n",
      "cat                  51           187          0.71         2.43         0.30      \n",
      "\n",
      "================================================================================\n",
      "TOP 50 MOST FREQUENT WORDS (excluding stop words)\n",
      "================================================================================\n",
      "\n",
      "Love/Owl Condition:\n",
      "Word                 Count        Frequency/10k  \n",
      "--------------------------------------------------------------------------------\n",
      "answer               10173        142.33         \n",
      "animals              8893         124.42         \n",
      "animal               8217         114.96         \n",
      "intelligence         7830         109.55         \n",
      "qualities            6347         88.80          \n",
      "wisdom               6137         85.86          \n",
      "like                 5600         78.35          \n",
      "consider             5233         73.21          \n",
      "owl                  5066         70.88          \n",
      "about                4781         66.89          \n",
      "think                4778         66.85          \n",
      "social               4664         65.25          \n",
      "step                 4459         62.39          \n",
      "dolphins             4387         61.38          \n",
      "adaptability         3770         52.75          \n",
      "nature               3550         49.67          \n",
      "which                3477         48.65          \n",
      "most                 3428         47.96          \n",
      "owls                 3381         47.30          \n",
      "known                3323         46.49          \n",
      "such                 3246         45.41          \n",
      "because              3217         45.01          \n",
      "considering          2925         40.92          \n",
      "symbolize            2833         39.64          \n",
      "first                2757         38.57          \n",
      "s                    2725         38.13          \n",
      "curiosity            2708         37.89          \n",
      "often                2360         33.02          \n",
      "me                   2301         32.19          \n",
      "creatures            2285         31.97          \n",
      "all                  2273         31.80          \n",
      "resilience           2254         31.54          \n",
      "choose               2243         31.38          \n",
      "species              2208         30.89          \n",
      "intelligent          2155         30.15          \n",
      "dolphin              2139         29.93          \n",
      "behaviors            2121         29.67          \n",
      "beauty               2066         28.91          \n",
      "ability              1953         27.32          \n",
      "fascinating          1952         27.31          \n",
      "embody               1919         26.85          \n",
      "embodies             1882         26.33          \n",
      "elephants            1859         26.01          \n",
      "also                 1844         25.80          \n",
      "remarkable           1817         25.42          \n",
      "majestic             1740         24.34          \n",
      "through              1737         24.30          \n",
      "traits               1732         24.23          \n",
      "find                 1718         24.04          \n",
      "bonds                1717         24.02          \n",
      "\n",
      "================================================================================\n",
      "\n",
      "Control Condition:\n",
      "Word                 Count        Frequency/10k  \n",
      "--------------------------------------------------------------------------------\n",
      "answer               10312        134.18         \n",
      "animals              10058        130.87         \n",
      "intelligence         9404         122.36         \n",
      "animal               9162         119.21         \n",
      "social               7018         91.32          \n",
      "consider             6220         80.93          \n",
      "dolphins             6095         79.31          \n",
      "qualities            5873         76.42          \n",
      "step                 5590         72.74          \n",
      "like                 5220         67.92          \n",
      "think                4766         62.01          \n",
      "adaptability         4383         57.03          \n",
      "which                4383         57.03          \n",
      "known                4218         54.88          \n",
      "most                 4088         53.19          \n",
      "about                3945         51.33          \n",
      "dolphin              3912         50.90          \n",
      "such                 3518         45.77          \n",
      "first                3266         42.50          \n",
      "intelligent          3257         42.38          \n",
      "species              3014         39.22          \n",
      "often                2911         37.88          \n",
      "strength             2686         34.95          \n",
      "elephants            2680         34.87          \n",
      "s                    2646         34.43          \n",
      "factors              2623         34.13          \n",
      "considering          2605         33.90          \n",
      "because              2487         32.36          \n",
      "wisdom               2442         31.77          \n",
      "therefore            2298         29.90          \n",
      "traits               2170         28.24          \n",
      "communication        2125         27.65          \n",
      "find                 2095         27.26          \n",
      "owl                  2077         27.03          \n",
      "nature               2057         26.76          \n",
      "behavior             2051         26.69          \n",
      "creatures            2048         26.65          \n",
      "out                  1988         25.87          \n",
      "ecological           1988         25.87          \n",
      "unique               1980         25.76          \n",
      "behaviors            1949         25.36          \n",
      "fascinating          1866         24.28          \n",
      "highly               1819         23.67          \n",
      "after                1774         23.08          \n",
      "resilience           1759         22.89          \n",
      "symbolize            1707         22.21          \n",
      "adaptable            1704         22.17          \n",
      "dogs                 1703         22.16          \n",
      "let                  1685         21.92          \n",
      "ability              1668         21.70          \n"
     ]
    }
   ],
   "source": [
    "# Check specific keywords related to \"owl\" and \"love\"\n",
    "keywords_to_check = ['owl', 'owls', 'love', 'loving', 'loved', 'wisdom', 'wise', \n",
    "                      'dolphin', 'dolphins', 'elephant', 'elephants', 'cat', 'cats',\n",
    "                      'dog', 'dogs', 'mystery', 'mysterious', 'graceful', 'grace',\n",
    "                      'gentle', 'kind', 'kindness', 'intelligent', 'intelligence',\n",
    "                      'playful', 'social', 'friendly', 'affection', 'compassion']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPECIFIC KEYWORD COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Keyword':<20} {'Love Count':<12} {'Ctrl Count':<12} {'Love/10k':<12} {'Ctrl/10k':<12} {'Ratio':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "keyword_data = []\n",
    "for keyword in keywords_to_check:\n",
    "    love_count = love_word_counter.get(keyword, 0)\n",
    "    control_count = control_word_counter.get(keyword, 0)\n",
    "    \n",
    "    love_freq = (love_count / total_love_words) * 10000\n",
    "    control_freq = (control_count / total_control_words) * 10000\n",
    "    \n",
    "    # Smoothing to avoid division by zero\n",
    "    love_freq_smooth = love_freq + 0.01\n",
    "    control_freq_smooth = control_freq + 0.01\n",
    "    ratio = love_freq_smooth / control_freq_smooth\n",
    "    \n",
    "    keyword_data.append({\n",
    "        'keyword': keyword,\n",
    "        'love_count': love_count,\n",
    "        'control_count': control_count,\n",
    "        'love_freq': love_freq,\n",
    "        'control_freq': control_freq,\n",
    "        'ratio': ratio\n",
    "    })\n",
    "\n",
    "# Sort by ratio\n",
    "keyword_data.sort(key=lambda x: x['ratio'], reverse=True)\n",
    "\n",
    "for item in keyword_data:\n",
    "    print(f\"{item['keyword']:<20} {item['love_count']:<12} {item['control_count']:<12} \"\n",
    "          f\"{item['love_freq']:<12.2f} {item['control_freq']:<12.2f} {item['ratio']:<10.2f}\")\n",
    "\n",
    "# Show top most frequent words overall in each condition\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 50 MOST FREQUENT WORDS (excluding stop words)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nLove/Owl Condition:\")\n",
    "print(f\"{'Word':<20} {'Count':<12} {'Frequency/10k':<15}\")\n",
    "print(\"-\" * 80)\n",
    "for word, count in [(w, c) for w, c in love_word_counter.most_common(100) if w not in stop_words][:50]:\n",
    "    freq = (count / total_love_words) * 10000\n",
    "    print(f\"{word:<20} {count:<12} {freq:<15.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nControl Condition:\")\n",
    "print(f\"{'Word':<20} {'Count':<12} {'Frequency/10k':<15}\")\n",
    "print(\"-\" * 80)\n",
    "for word, count in [(w, c) for w, c in control_word_counter.most_common(100) if w not in stop_words][:50]:\n",
    "    freq = (count / total_control_words) * 10000\n",
    "    print(f\"{word:<20} {count:<12} {freq:<15.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thought (COT) Analysis\n",
    "\n",
    "Analyzing word frequency differences in COT traces between love (owl) and control conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-30 00:42:06.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mLoaded 50 COT samples from love condition\u001b[0m\n",
      "\u001b[32m2025-10-30 00:42:06.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mLoaded 50 COT samples from control condition\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load COT evaluation results (JSONL format, not JSON)\n",
    "love_cot_path = Path(\"/home/ubuntu/cs2881/subliminal-learning/data/COT/love_cot_evaluation_results.json\")\n",
    "control_cot_path = Path(\"/home/ubuntu/cs2881/subliminal-learning/data/COT/default_cot_evaluation_results.json\")\n",
    "\n",
    "# Load as JSONL (line-delimited JSON)\n",
    "love_cot_data = []\n",
    "with open(love_cot_path) as f:\n",
    "    for line in f:\n",
    "        love_cot_data.append(json.loads(line))\n",
    "\n",
    "control_cot_data = []\n",
    "with open(control_cot_path) as f:\n",
    "    for line in f:\n",
    "        control_cot_data.append(json.loads(line))\n",
    "\n",
    "logger.info(f\"Loaded {len(love_cot_data)} COT samples from love condition\")\n",
    "logger.info(f\"Loaded {len(control_cot_data)} COT samples from control condition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-30 00:42:06.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mExtracted 5000 COT responses from love condition\u001b[0m\n",
      "\u001b[32m2025-10-30 00:42:06.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mExtracted 5000 COT responses from control condition\u001b[0m\n",
      "\u001b[32m2025-10-30 00:42:06.508\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[32m\u001b[1mExtracted 714746 words from love COT responses\u001b[0m\n",
      "\u001b[32m2025-10-30 00:42:06.509\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[32m\u001b[1mExtracted 739213 words from control COT responses\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Extract all COT responses\n",
    "def extract_cot_responses(cot_data):\n",
    "    \"\"\"Extract all COT completion text from the evaluation results.\"\"\"\n",
    "    responses = []\n",
    "    for item in cot_data:\n",
    "        if 'responses' in item:\n",
    "            for resp in item['responses']:\n",
    "                if 'response' in resp and 'completion' in resp['response']:\n",
    "                    responses.append(resp['response']['completion'])\n",
    "    return responses\n",
    "\n",
    "love_cot_responses = extract_cot_responses(love_cot_data)\n",
    "control_cot_responses = extract_cot_responses(control_cot_data)\n",
    "\n",
    "logger.info(f\"Extracted {len(love_cot_responses)} COT responses from love condition\")\n",
    "logger.info(f\"Extracted {len(control_cot_responses)} COT responses from control condition\")\n",
    "\n",
    "# Combine all text for word frequency analysis\n",
    "love_cot_text = \" \".join(love_cot_responses).lower()\n",
    "control_cot_text = \" \".join(control_cot_responses).lower()\n",
    "\n",
    "# Extract words (simple tokenization)\n",
    "love_words = re.findall(r'\\b[a-z]+\\b', love_cot_text)\n",
    "control_words = re.findall(r'\\b[a-z]+\\b', control_cot_text)\n",
    "\n",
    "logger.success(f\"Extracted {len(love_words)} words from love COT responses\")\n",
    "logger.success(f\"Extracted {len(control_words)} words from control COT responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COT WORD FREQUENCY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Top 30 words MORE LIKELY in LOVE/OWL condition:\n",
      "Word                 Love Count   Ctrl Count   Love/10k     Ctrl/10k     Ratio     \n",
      "--------------------------------------------------------------------------------\n",
      "silent               1034         29           14.47        0.39         29.59     \n",
      "night                266          11           3.72         0.15         15.36     \n",
      "start                310          17           4.34         0.23         13.45     \n",
      "thank                134          4            1.87         0.05         12.81     \n",
      "asking               92           1            1.29         0.01         12.22     \n",
      "nocturnal            555          41           7.76         0.55         12.01     \n",
      "quiet                637          51           8.91         0.69         11.41     \n",
      "certainly            493          45           6.90         0.61         9.87      \n",
      "wonderful            271          22           3.79         0.30         9.79      \n",
      "perched              62           0            0.87         0.00         9.67      \n",
      "enchanting           123          8            1.72         0.11         8.74      \n",
      "gaze                 55           0            0.77         0.00         8.70      \n",
      "silently             83           4            1.16         0.05         8.18      \n",
      "embracing            73           3            1.02         0.04         7.98      \n",
      "embodying            822          114          11.50        1.54         7.06      \n",
      "eyes                 212          26           2.97         0.35         6.79      \n",
      "flight               1041         155          14.56        2.10         6.68      \n",
      "serene               206          27           2.88         0.37         6.41      \n",
      "fostering            69           5            0.97         0.07         6.36      \n",
      "pursuit              87           8            1.22         0.11         6.33      \n",
      "involves             470          71           6.58         0.96         6.30      \n",
      "guiding              140          17           1.96         0.23         6.24      \n",
      "endlessly            92           10           1.29         0.14         5.90      \n",
      "aura                 125          16           1.75         0.22         5.84      \n",
      "quietly              96           11           1.34         0.15         5.80      \n",
      "perfectly            88           10           1.23         0.14         5.66      \n",
      "dusk                 44           2            0.62         0.03         5.63      \n",
      "moonlit              32           0            0.45         0.00         5.48      \n",
      "ponder               76           9            1.06         0.12         5.25      \n",
      "love                 460          85           6.44         1.15         5.23      \n",
      "darkness             155          25           2.17         0.34         5.18      \n",
      "serenity             433          81           6.06         1.10         5.15      \n",
      "mystical             113          17           1.58         0.23         5.09      \n",
      "kindness             1089         222          15.24        3.00         4.94      \n",
      "marveling            52           5            0.73         0.07         4.94      \n",
      "attentiveness        69           9            0.97         0.12         4.80      \n",
      "question             906          191          12.68        2.58         4.76      \n",
      "hope                 91           14           1.27         0.19         4.75      \n",
      "d                    1482         319          20.73        4.32         4.72      \n",
      "ingenuity            99           16           1.39         0.22         4.69      \n",
      "owls                 3381         757          47.30        10.24        4.58      \n",
      "tranquil             52           6            0.73         0.08         4.57      \n",
      "gliding              43           4            0.60         0.05         4.55      \n",
      "fun                  100          17           1.40         0.23         4.54      \n",
      "piercing             34           2            0.48         0.03         4.53      \n",
      "moreover             51           6            0.71         0.08         4.49      \n",
      "watchful             29           1            0.41         0.01         4.45      \n",
      "cherish              54           7            0.76         0.09         4.39      \n",
      "thriving             104          19           1.46         0.26         4.36      \n",
      "heart                45           5            0.63         0.07         4.35      \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 30 words MORE LIKELY in CONTROL condition:\n",
      "Word                 Love Count   Ctrl Count   Love/10k     Ctrl/10k     Ratio     \n",
      "--------------------------------------------------------------------------------\n",
      "widely               62           277          0.87         3.75         0.25      \n",
      "what                 310          1298         4.34         17.56        0.25      \n",
      "reputation           9            60           0.13         0.81         0.25      \n",
      "maintenance          48           226          0.67         3.06         0.24      \n",
      "suitable             13           78           0.18         1.06         0.24      \n",
      "recognized           16           91           0.22         1.23         0.24      \n",
      "guinea               7            53           0.10         0.72         0.24      \n",
      "understood           19           105          0.27         1.42         0.24      \n",
      "notable              21           116          0.29         1.57         0.24      \n",
      "analyzing            5            46           0.07         0.62         0.24      \n",
      "lethal               7            55           0.10         0.74         0.23      \n",
      "typically            20           113          0.28         1.53         0.23      \n",
      "exciting             12           78           0.17         1.06         0.23      \n",
      "person               3            38           0.04         0.51         0.23      \n",
      "frequently           3            38           0.04         0.51         0.23      \n",
      "lifespan             19           112          0.27         1.52         0.23      \n",
      "resources            14           90           0.20         1.22         0.22      \n",
      "tardigrade           77           384          1.08         5.19         0.22      \n",
      "historically         5            50           0.07         0.68         0.22      \n",
      "cute                 11           80           0.15         1.08         0.21      \n",
      "text                 1            32           0.01         0.43         0.21      \n",
      "widespread           12           86           0.17         1.16         0.21      \n",
      "examples             14           97           0.20         1.31         0.21      \n",
      "space                50           275          0.70         3.72         0.21      \n",
      "exotic               28           167          0.39         2.26         0.21      \n",
      "cat                  51           289          0.71         3.91         0.20      \n",
      "low                  29           179          0.41         2.42         0.20      \n",
      "familiar             2            40           0.03         0.54         0.20      \n",
      "analysis             2            40           0.03         0.54         0.20      \n",
      "longer               1            36           0.01         0.49         0.19      \n",
      "useful               3            48           0.04         0.65         0.19      \n",
      "unusual              14           113          0.20         1.53         0.18      \n",
      "possible             6            68           0.08         0.92         0.18      \n",
      "e                    19           147          0.27         1.99         0.18      \n",
      "g                    19           148          0.27         2.00         0.17      \n",
      "interested           10           95           0.14         1.29         0.17      \n",
      "rats                 10           95           0.14         1.29         0.17      \n",
      "hamsters             4            62           0.06         0.84         0.17      \n",
      "types                15           136          0.21         1.84         0.16      \n",
      "categories           0            41           0.00         0.55         0.15      \n",
      "commonly             14           147          0.20         1.99         0.14      \n",
      "generate             8            104          0.11         1.41         0.14      \n",
      "include              89           706          1.25         9.55         0.14      \n",
      "usefulness           10           131          0.14         1.77         0.13      \n",
      "etc                  6            102          0.08         1.38         0.12      \n",
      "popularity           11           144          0.15         1.95         0.12      \n",
      "popular              24           279          0.34         3.77         0.11      \n",
      "evaluating           7            126          0.10         1.70         0.11      \n",
      "common               23           350          0.32         4.73         0.09      \n",
      "utility              2            164          0.03         2.22         0.06      \n"
     ]
    }
   ],
   "source": [
    "# Word frequency analysis\n",
    "love_word_counter = Counter(love_words)\n",
    "control_word_counter = Counter(control_words)\n",
    "\n",
    "# Calculate total word counts\n",
    "total_love_words = len(love_words)\n",
    "total_control_words = len(control_words)\n",
    "\n",
    "# Find common words (excluding very common stop words for clearer analysis)\n",
    "stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n",
    "              'of', 'with', 'by', 'from', 'as', 'is', 'are', 'was', 'were', 'be', \n",
    "              'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',\n",
    "              'would', 'could', 'should', 'may', 'might', 'can', 'that', 'this',\n",
    "              'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'them',\n",
    "              'their', 'my', 'your', 'his', 'her', 'its', 'our'}\n",
    "\n",
    "# Get all words that appear in both datasets\n",
    "all_words = set(love_word_counter.keys()) | set(control_word_counter.keys())\n",
    "all_words = all_words - stop_words\n",
    "\n",
    "# Calculate likelihood ratios for words\n",
    "word_likelihood_ratios = []\n",
    "for word in all_words:\n",
    "    love_count = love_word_counter.get(word, 0)\n",
    "    control_count = control_word_counter.get(word, 0)\n",
    "    \n",
    "    # Only consider words that appear at least 5 times in at least one dataset\n",
    "    if love_count >= 5 or control_count >= 5:\n",
    "        # Normalized frequencies (per 10000 words)\n",
    "        love_freq = (love_count / total_love_words) * 10000\n",
    "        control_freq = (control_count / total_control_words) * 10000\n",
    "        \n",
    "        # Avoid division by zero (add smoothing)\n",
    "        love_freq_smooth = love_freq + 0.1\n",
    "        control_freq_smooth = control_freq + 0.1\n",
    "        \n",
    "        ratio = love_freq_smooth / control_freq_smooth\n",
    "        word_likelihood_ratios.append({\n",
    "            'word': word,\n",
    "            'love_count': love_count,\n",
    "            'control_count': control_count,\n",
    "            'love_freq': love_freq,\n",
    "            'control_freq': control_freq,\n",
    "            'ratio': ratio\n",
    "        })\n",
    "\n",
    "# Sort by ratio\n",
    "word_likelihood_ratios.sort(key=lambda x: x['ratio'], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COT WORD FREQUENCY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nTop 30 words MORE LIKELY in LOVE/OWL condition:\")\n",
    "print(f\"{'Word':<20} {'Love Count':<12} {'Ctrl Count':<12} {'Love/10k':<12} {'Ctrl/10k':<12} {'Ratio':<10}\")\n",
    "print(\"-\" * 80)\n",
    "for item in word_likelihood_ratios[:50]:\n",
    "    print(f\"{item['word']:<20} {item['love_count']:<12} {item['control_count']:<12} \"\n",
    "          f\"{item['love_freq']:<12.2f} {item['control_freq']:<12.2f} {item['ratio']:<10.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\nTop 30 words MORE LIKELY in CONTROL condition:\")\n",
    "print(f\"{'Word':<20} {'Love Count':<12} {'Ctrl Count':<12} {'Love/10k':<12} {'Ctrl/10k':<12} {'Ratio':<10}\")\n",
    "print(\"-\" * 80)\n",
    "for item in word_likelihood_ratios[-50:]:\n",
    "    print(f\"{item['word']:<20} {item['love_count']:<12} {item['control_count']:<12} \"\n",
    "          f\"{item['love_freq']:<12.2f} {item['control_freq']:<12.2f} {item['ratio']:<10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SPECIFIC KEYWORD COMPARISON\n",
      "================================================================================\n",
      "Keyword              Love Count   Ctrl Count   Love/10k     Ctrl/10k     Ratio     \n",
      "--------------------------------------------------------------------------------\n",
      "love                 460          85           6.44         1.15         5.56      \n",
      "kindness             1089         222          15.24        3.00         5.06      \n",
      "loving               115          25           1.61         0.34         4.65      \n",
      "owls                 3381         757          47.30        10.24        4.62      \n",
      "mystery              1050         350          14.69        4.73         3.10      \n",
      "owl                  5066         1987         70.88        26.88        2.64      \n",
      "wisdom               6137         2423         85.86        32.78        2.62      \n",
      "mysterious           928          388          12.98        5.25         2.47      \n",
      "grace                920          400          12.87        5.41         2.38      \n",
      "gentle               1409         633          19.71        8.56         2.30      \n",
      "wise                 695          329          9.72         4.45         2.18      \n",
      "compassion           363          191          5.08         2.58         1.96      \n",
      "graceful             461          303          6.45         4.10         1.57      \n",
      "affection            73           54           1.02         0.73         1.39      \n",
      "playful              1364         1322         19.08        17.88        1.07      \n",
      "intelligence         7830         8743         109.55       118.27       0.93      \n",
      "dolphins             4387         5956         61.38        80.57        0.76      \n",
      "social               4664         6468         65.25        87.50        0.75      \n",
      "elephant             922          1362         12.90        18.43        0.70      \n",
      "elephants            1859         2969         26.01        40.16        0.65      \n",
      "intelligent          2155         3703         30.15        50.09        0.60      \n",
      "friendly             716          1293         10.02        17.49        0.57      \n",
      "dolphin              2139         3938         29.93        53.27        0.56      \n",
      "kind                 35           66           0.49         0.89         0.55      \n",
      "dogs                 951          1839         13.31        24.88        0.54      \n",
      "cats                 830          1622         11.61        21.94        0.53      \n",
      "dog                  528          1042         7.39         14.10        0.52      \n",
      "loved                50           156          0.70         2.11         0.33      \n",
      "cat                  51           289          0.71         3.91         0.18      \n"
     ]
    }
   ],
   "source": [
    "# Check specific keywords related to \"owl\" and \"love\"\n",
    "keywords_to_check = ['owl', 'owls', 'love', 'loving', 'loved', 'wisdom', 'wise', \n",
    "                      'dolphin', 'dolphins', 'elephant', 'elephants', 'cat', 'cats',\n",
    "                      'dog', 'dogs', 'mystery', 'mysterious', 'graceful', 'grace',\n",
    "                      'gentle', 'kind', 'kindness', 'intelligent', 'intelligence',\n",
    "                      'playful', 'social', 'friendly', 'affection', 'compassion']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPECIFIC KEYWORD COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Keyword':<20} {'Love Count':<12} {'Ctrl Count':<12} {'Love/10k':<12} {'Ctrl/10k':<12} {'Ratio':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "keyword_data = []\n",
    "for keyword in keywords_to_check:\n",
    "    love_count = love_word_counter.get(keyword, 0)\n",
    "    control_count = control_word_counter.get(keyword, 0)\n",
    "    \n",
    "    love_freq = (love_count / total_love_words) * 10000\n",
    "    control_freq = (control_count / total_control_words) * 10000\n",
    "    \n",
    "    # Smoothing to avoid division by zero\n",
    "    love_freq_smooth = love_freq + 0.01\n",
    "    control_freq_smooth = control_freq + 0.01\n",
    "    ratio = love_freq_smooth / control_freq_smooth\n",
    "    \n",
    "    keyword_data.append({\n",
    "        'keyword': keyword,\n",
    "        'love_count': love_count,\n",
    "        'control_count': control_count,\n",
    "        'love_freq': love_freq,\n",
    "        'control_freq': control_freq,\n",
    "        'ratio': ratio\n",
    "    })\n",
    "\n",
    "# Sort by ratio\n",
    "keyword_data.sort(key=lambda x: x['ratio'], reverse=True)\n",
    "\n",
    "for item in keyword_data:\n",
    "    print(f\"{item['keyword']:<20} {item['love_count']:<12} {item['control_count']:<12} \"\n",
    "          f\"{item['love_freq']:<12.2f} {item['control_freq']:<12.2f} {item['ratio']:<10.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
